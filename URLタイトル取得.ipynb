{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPo9dnp5aKEQ"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def get_title_text(HOST_URL:str='https://github.com',\n",
        "                   page_path:str='',\n",
        "                   *,\n",
        "                   response_decode:str='utf-8',\n",
        "                   time_out:int=20,\n",
        "                   time_sleep:int=1) -> str:\n",
        "  # Referrer\n",
        "  # https://scrapeops.io/web-scraping-playbook/403-forbidden-error-web-scraping/\n",
        "  headers = {\n",
        "      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0',\n",
        "      'referer': HOST_URL,\n",
        "      \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
        "      \"Accept-Language\": \"ja,en-US;q=0.9,en;q=0.8\",\n",
        "      'Accept-Charset': response_decode,\n",
        "      \"Accept-Encoding\": response_decode,\n",
        "      \"Connection\": \"keep-alive\",\n",
        "      \"Upgrade-Insecure-Requests\": \"1\",\n",
        "      \"Sec-Fetch-Dest\": \"document\",\n",
        "      \"Sec-Fetch-Mode\": \"navigate\",\n",
        "      \"Sec-Fetch-Site\": \"none\",\n",
        "      \"Sec-Fetch-User\": \"?1\",\n",
        "      \"Cache-Control\": \"max-age=0\",\n",
        "  }\n",
        "\n",
        "  try:\n",
        "    request = urllib.request.Request(url=HOST_URL+page_path, headers=headers)\n",
        "    response = urllib.request.urlopen(request, timeout=time_out)\n",
        "    html = response.read().decode(response_decode)\n",
        "    soup  = BeautifulSoup(html, 'html.parser')\n",
        "    title_text = soup.title.getText()\n",
        "  except:\n",
        "    title_text = '***'\n",
        "\n",
        "  # アクセス負荷軽減\n",
        "  time.sleep(time_sleep)\n",
        "\n",
        "  return title_text\n",
        "\n",
        "def func(page_path:str='') -> str:\n",
        "  title_text = get_title_text(page_path=page_path)\n",
        "  return title_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4QztojD-dKkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(['/','/features/actions','/404'], columns=['path'])\n",
        "\n",
        "df = df.assign(title_text = lambda x: x.path.apply(func))\n",
        "df"
      ],
      "metadata": {
        "id": "i8bDDr8tdKh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HjR-yXUBaN_-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}